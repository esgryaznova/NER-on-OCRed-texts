{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6acfc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d3a1bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_df = pd.read_excel(\"spacy_df.xlsx\")\n",
    "reg_rec = pd.read_excel(\"Desktop/reg_rec.xlsx\")\n",
    "ner_labeling = pd.read_excel(\"Desktop/ner_labeling.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f2a46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_df = spacy_df.drop(\"Unnamed: 0\", axis=1)\n",
    "reg_rec = reg_rec.drop(\"Unnamed: 0\", axis=1)\n",
    "ner_labeling = ner_labeling.drop(\"Unnamed: 0\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_values = []\n",
    "# for i in ner_labeling[\"Label\"]:\n",
    "#     i = re.sub(r'[\\W_]', '', i)\n",
    "#     clean_values.append(i)\n",
    "\n",
    "# ner_labeling[\"Label\"] = clean_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6281cdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_labeling_ORG = ner_labeling[ner_labeling.Label == \"ORG\"]\n",
    "ner_labeling_PER = ner_labeling[ner_labeling.Label == \"PER\"]\n",
    "ner_labeling_LOC = ner_labeling[ner_labeling.Label == \"LOC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3188cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_labeling_clean = pd.concat([ner_labeling_ORG, ner_labeling_PER, ner_labeling_LOC], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4937cfa",
   "metadata": {},
   "source": [
    "## SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1634141d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two dataframes have 5124 identical rows.\n"
     ]
    }
   ],
   "source": [
    "# merge the dataframes on the 'filenames' column\n",
    "merged_df = pd.merge(spacy_df, ner_labeling_clean, how = \"inner\")\n",
    "\n",
    "# count the number of rows in the merged dataframe (which will be the number of identical rows)\n",
    "num_identical_rows = len(merged_df)\n",
    "\n",
    "print(f'The two dataframes have {num_identical_rows} identical rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "268632cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two dataframes have 197 rows with the same \"text\" value but different \"labels\" values.\n"
     ]
    }
   ],
   "source": [
    "# same entities, different tags\n",
    "diff_merged_df = pd.merge(spacy_df, ner_labeling_clean, on=['Filename','Text'])\n",
    "\n",
    "# filter the merged dataframe to include only rows where the 'labels' values are different\n",
    "diff_labels_df = diff_merged_df[diff_merged_df['Label_x'] != diff_merged_df['Label_y']]\n",
    "\n",
    "# count the number of rows in the filtered dataframe (which will be the number of rows with the same 'text' value but different 'labels' values)\n",
    "num_diff_rows = len(diff_labels_df)\n",
    "\n",
    "print(f'The two dataframes have {num_diff_rows} rows with the same \"text\" value but different \"labels\" values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77af09bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.82      0.91      0.87       380\n",
      "         ORG       0.96      1.00      0.98      2719\n",
      "         PER       1.00      0.93      0.96      2222\n",
      "\n",
      "    accuracy                           0.96      5321\n",
      "   macro avg       0.93      0.95      0.94      5321\n",
      "weighted avg       0.96      0.96      0.96      5321\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Metrics compared to only 5124 entities of the same boundaries (could be different tags)\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(diff_merged_df[\"Label_y\"], diff_merged_df[\"Label_x\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00374313",
   "metadata": {},
   "source": [
    "Strict predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d2b83144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Доля 5124 (правильно найденных спейси) / 35706 (все что нашел спейси)\n",
    "precision = 0.14350529\n",
    "\n",
    "# Доля 5124 (правильных найденных спейси)/ 17925 (все которые нужно было найти)\n",
    "recall = 0.28585774"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ecc689df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19108351213864222"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53e16b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_org = len(merged_df[merged_df.Label == \"ORG\"]) / len(spacy_df[spacy_df.Label == \"ORG\"])\n",
    "recall_org = len(merged_df[merged_df.Label == \"ORG\"]) / len(ner_labeling_clean[ner_labeling_clean.Label == \"ORG\"])\n",
    "f1_org = 2 * (precision_org * recall_org) / (precision_org + recall_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b60395e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6403"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ner_labeling_clean[ner_labeling_clean.Label == \"ORG\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8c1e70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18222222222222223 0.4226143995002343 0.2546464028607726\n"
     ]
    }
   ],
   "source": [
    "print(precision_org, recall_org, f1_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7b36dead",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_loc = len(merged_df[merged_df.Label == \"LOC\"]) / len(spacy_df[spacy_df.Label == \"LOC\"])\n",
    "recall_loc = len(merged_df[merged_df.Label == \"LOC\"]) / len(ner_labeling_clean[ner_labeling_clean.Label == \"LOC\"])\n",
    "f1_loc = 2 * (precision_loc * recall_loc) / (precision_loc + recall_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b39c9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02722422720853601 0.06156848828956707 0.03775432488303775\n"
     ]
    }
   ],
   "source": [
    "print(precision_loc, recall_loc, f1_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "78b5de67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5636"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ner_labeling_clean[ner_labeling_clean.Label == \"LOC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad8cfc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_per = len(merged_df[merged_df.Label == \"PER\"]) / len(spacy_df[spacy_df.Label == \"PER\"])\n",
    "recall_per = len(merged_df[merged_df.Label == \"PER\"]) / len(ner_labeling_clean[ner_labeling_clean.Label == \"PER\"])\n",
    "f1_per = 2 * (precision_per * recall_per) / (precision_per + recall_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "098ba074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.255363748458693 0.35185185185185186 0.2959416976278937\n"
     ]
    }
   ],
   "source": [
    "print(precision_per, recall_per, f1_per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4dc99a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5886"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ner_labeling_clean[ner_labeling_clean.Label == \"PER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67d1f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "def match_entities(df_ann, df_dp):\n",
    "    matches = []\n",
    "    for _, row_ann in df_ann.iterrows():\n",
    "        for _, row_dp in df_dp.iterrows():\n",
    "            if row_ann['Filename'] == row_dp['Filename'] and row_ann['Label'] == row_dp['Label'] and fuzz.ratio(row_ann['Text'], row_dp['Text']) >= 80:\n",
    "                matches.append((row_ann.name, row_dp.name))\n",
    "                print(len(matches))\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14274c18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spacy_matches = match_entities(ner_labeling_clean, spacy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "78d6cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = len(spacy_matches)\n",
    "fp = len(dp_df) - tp\n",
    "fn = len(ner_labeling_clean) - tp\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "3d6a615a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: ORG\n",
      "Precision: 0.492492624719299\n",
      "Recall: 0.8176169590643275\n",
      "F1 score: 0.6147124288972549\n",
      "Class: LOC\n",
      "Precision: 0.19614243323442138\n",
      "Recall: 0.3581685180167976\n",
      "F1 score: 0.25347521809989454\n",
      "Class: PER\n",
      "Precision: 0.5477722979185823\n",
      "Recall: 0.7360440301022128\n",
      "F1 score: 0.6281031342854404\n",
      "Overall precision: 0.4232670183162354\n",
      "Overall recall: 0.6801935591523444\n",
      "Overall F1 score: 0.5218192757203753\n"
     ]
    }
   ],
   "source": [
    "# Define the classes to calculate metrics for\n",
    "classes = [\"ORG\", \"LOC\", \"PER\"]\n",
    "\n",
    "# Calculate true positives, false positives, and false negatives for each class\n",
    "tp_by_class = {c: 0 for c in classes}\n",
    "fp_by_class = {c: 0 for c in classes}\n",
    "fn_by_class = {c: 0 for c in classes}\n",
    "for i, row in ner_labeling_clean.iterrows():\n",
    "    match_found = False\n",
    "    for match in spacy_matches:\n",
    "        if match[0] == i:\n",
    "            match_found = True\n",
    "            j = match[1]\n",
    "            if row[\"Label\"] == spacy_df.iloc[j][\"Label\"]:\n",
    "                tp_by_class[row[\"Label\"]] += 1\n",
    "            else:\n",
    "                fn_by_class[row[\"Label\"]] += 1\n",
    "    if not match_found:\n",
    "        fn_by_class[row[\"Label\"]] += 1\n",
    "\n",
    "for j, row in spacy_df.iterrows():\n",
    "    match_found = False\n",
    "    for match in spacy_matches:\n",
    "        if match[1] == j:\n",
    "            match_found = True\n",
    "    if not match_found:\n",
    "        fp_by_class[row[\"Label\"]] += 1\n",
    "\n",
    "# Calculate precision, recall, and F1 score for each class\n",
    "for c in classes:\n",
    "    precision = tp_by_class[c] / (tp_by_class[c] + fp_by_class[c])\n",
    "    recall = tp_by_class[c] / (tp_by_class[c] + fn_by_class[c])\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    print(\"Class:\", c)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 score:\", f1_score)\n",
    "\n",
    "# Calculate overall precision, recall, and F1 score\n",
    "tp = sum(tp_by_class.values())\n",
    "fp = sum(fp_by_class.values())\n",
    "fn = sum(fn_by_class.values())\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(\"Overall precision:\", precision)\n",
    "print(\"Overall recall:\", recall)\n",
    "print(\"Overall F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf9ed18",
   "metadata": {},
   "source": [
    "## Natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5210d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "natasha = json.load(open('Desktop/natasha/natasha.json', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "3f002f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "# loop through the dictionary\n",
    "for filename, (text, tags) in natasha.items():\n",
    "    # create a row with the filename, text, and tags\n",
    "    for i in range(len(text)):\n",
    "        row = [filename, text[i], tags[i]]\n",
    "        # add the row to the list of rows\n",
    "        rows.append(row)\n",
    "\n",
    "# create a dataframe from the list of rows\n",
    "natasha_df = pd.DataFrame(rows, columns=['Filename', 'Text', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "deeb1e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two dataframes have 7312 identical rows.\n"
     ]
    }
   ],
   "source": [
    "# merge the dataframes on the 'filenames' column\n",
    "nat_df_merged = pd.merge(natasha_df, ner_labeling_clean, how = \"inner\")\n",
    "\n",
    "# count the number of rows in the merged dataframe (which will be the number of identical rows)\n",
    "num_identical_rows = len(nat_df_merged)\n",
    "\n",
    "print(f'The two dataframes have {num_identical_rows} identical rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ff243069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two dataframes have 88 rows with the same \"text\" value but different \"labels\" values.\n"
     ]
    }
   ],
   "source": [
    "# same entities, different tags\n",
    "nat_diff_merged_df = pd.merge(natasha_df, ner_labeling_clean, on=['Filename','Text'])\n",
    "\n",
    "# filter the merged dataframe to include only rows where the 'labels' values are different\n",
    "nat_diff_labels_df = nat_diff_merged_df[nat_diff_merged_df['Label_x'] != nat_diff_merged_df['Label_y']]\n",
    "# count the number of rows in the filtered dataframe (which will be the number of rows with the same 'text' value but different 'labels' values)\n",
    "num_diff_rows = len(nat_diff_labels_df)\n",
    "\n",
    "print(f'The two dataframes have {num_diff_rows} rows with the same \"text\" value but different \"labels\" values.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "131d7478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.98      0.93      0.95       390\n",
      "         ORG       0.97      1.00      0.98      2368\n",
      "         PER       1.00      0.99      0.99      4642\n",
      "\n",
      "    accuracy                           0.99      7400\n",
      "   macro avg       0.98      0.97      0.98      7400\n",
      "weighted avg       0.99      0.99      0.99      7400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lenient F1\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(nat_diff_merged_df[\"Label_y\"], nat_diff_merged_df[\"Label_x\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d7cdf7f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1651305683563748 0.3693581133843511 0.22822677925211093\n"
     ]
    }
   ],
   "source": [
    "precision_org = len(nat_df_merged[nat_df_merged.Label == \"ORG\"]) / len(natasha_df[natasha_df.Label == \"ORG\"])\n",
    "recall_org = len(nat_df_merged[nat_df_merged.Label == \"ORG\"]) / len(ner_labeling_clean[ner_labeling_clean.Label == \"ORG\"])\n",
    "f1_org = 2 * (precision_org * recall_org) / (precision_org + recall_org)\n",
    "print(precision_org, recall_org, f1_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "eac06491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6403"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ner_labeling_clean[ner_labeling_clean.Label == \"ORG\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bdf86cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02982499383781119 0.06440738112136267 0.040770483517717754\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5636"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_loc = len(nat_df_merged[nat_df_merged.Label == \"LOC\"]) / len(natasha_df[natasha_df.Label == \"LOC\"])\n",
    "recall_loc = len(nat_df_merged[nat_df_merged.Label == \"LOC\"]) / len(ner_labeling_clean[ner_labeling_clean.Label == \"LOC\"])\n",
    "f1_loc = 2 * (precision_loc * recall_loc) / (precision_loc + recall_loc)\n",
    "print(precision_loc, recall_loc, f1_loc)\n",
    "len(ner_labeling_clean[ner_labeling_clean.Label == \"LOC\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e9f5d132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6942298955020445 0.7787971457696228 0.7340859956761949\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5886"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_per = len(nat_df_merged[nat_df_merged.Label == \"PER\"]) / len(natasha_df[natasha_df.Label == \"PER\"])\n",
    "recall_per = len(nat_df_merged[nat_df_merged.Label == \"PER\"]) / len(ner_labeling_clean[ner_labeling_clean.Label == \"PER\"])\n",
    "f1_per = 2 * (precision_per * recall_per) / (precision_per + recall_per)\n",
    "print(precision_per, recall_per, f1_per)\n",
    "len(ner_labeling_clean[ner_labeling_clean.Label == \"PER\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16d00e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "natasha_matches = match_entities(ner_labeling_clean, natasha_df)\n",
    "\n",
    "tp = len(natasha_matches)\n",
    "fp = len(dp_df) - tp\n",
    "fn = len(ner_labeling_clean) - tp\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "e6a436e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: ORG\n",
      "Precision: 0.4790281808819399\n",
      "Recall: 0.8275109170305677\n",
      "F1 score: 0.606795540796964\n",
      "Class: LOC\n",
      "Precision: 0.2083236815689908\n",
      "Recall: 0.366227772498982\n",
      "F1 score: 0.2655773206024215\n",
      "Class: PER\n",
      "Precision: 0.7376906662451592\n",
      "Recall: 0.863619541080681\n",
      "F1 score: 0.7957035079493627\n",
      "Overall precision: 0.47406634586722307\n",
      "Overall recall: 0.7290200058937166\n",
      "Overall F1 score: 0.5745287522417329\n"
     ]
    }
   ],
   "source": [
    "# Define the classes to calculate metrics for\n",
    "classes = [\"ORG\", \"LOC\", \"PER\"]\n",
    "\n",
    "# Calculate true positives, false positives, and false negatives for each class\n",
    "tp_by_class = {c: 0 for c in classes}\n",
    "fp_by_class = {c: 0 for c in classes}\n",
    "fn_by_class = {c: 0 for c in classes}\n",
    "for i, row in ner_labeling_clean.iterrows():\n",
    "    match_found = False\n",
    "    for match in natasha_matches:\n",
    "        if match[0] == i:\n",
    "            match_found = True\n",
    "            j = match[1]\n",
    "            if row[\"Label\"] == natasha_df.iloc[j][\"Label\"]:\n",
    "                tp_by_class[row[\"Label\"]] += 1\n",
    "            else:\n",
    "                fn_by_class[row[\"Label\"]] += 1\n",
    "    if not match_found:\n",
    "        fn_by_class[row[\"Label\"]] += 1\n",
    "\n",
    "for j, row in natasha_df.iterrows():\n",
    "    match_found = False\n",
    "    for match in natasha_matches:\n",
    "        if match[1] == j:\n",
    "            match_found = True\n",
    "    if not match_found:\n",
    "        fp_by_class[row[\"Label\"]] += 1\n",
    "\n",
    "# Calculate precision, recall, and F1 score for each class\n",
    "for c in classes:\n",
    "    precision = tp_by_class[c] / (tp_by_class[c] + fp_by_class[c])\n",
    "    recall = tp_by_class[c] / (tp_by_class[c] + fn_by_class[c])\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    print(\"Class:\", c)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 score:\", f1_score)\n",
    "\n",
    "# Calculate overall precision, recall, and F1 score\n",
    "tp = sum(tp_by_class.values())\n",
    "fp = sum(fp_by_class.values())\n",
    "fn = sum(fn_by_class.values())\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(\"Overall precision:\", precision)\n",
    "print(\"Overall recall:\", recall)\n",
    "print(\"Overall F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadbdef2",
   "metadata": {},
   "source": [
    "## DeepPavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca08f91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "dp = json.load(open('Desktop/deep_pavlov.json', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3306c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "# loop through the dictionary\n",
    "for filename, (text, tags) in dp.items():\n",
    "    # create a row with the filename, text, and tags\n",
    "    for i in range(len(text)):\n",
    "        row = [filename, text[i], tags[i]]\n",
    "        # add the row to the list of rows\n",
    "        rows.append(row)\n",
    "\n",
    "# create a dataframe from the list of rows\n",
    "dp_df = pd.DataFrame(rows, columns=['Filename', 'Text', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8219a960",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_df = dp_df.explode(['Text', 'Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29de367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dp_df = dp_df[dp_df.Label != \"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b5e1759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "dp_df.to_excel('dp_df_full.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1c60eceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat entities\n",
    "dp_df['Entity'] = dp_df['Label'].apply(lambda x: x.split('-')[1] if '-' in x else '')\n",
    "\n",
    "# Group by filename and entity, and concatenate the 'text' and 'label' columns\n",
    "dp_df = dp_df.groupby(['Filename', 'Entity']).agg({'Text': ' '.join, 'Label': 'first'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "614044d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two dataframes have 0 identical rows.\n"
     ]
    }
   ],
   "source": [
    "# merge the dataframes on the 'filenames' column\n",
    "dp_df_merged = pd.merge(dp_df, ner_labeling_clean, how = \"inner\")\n",
    "\n",
    "# count the number of rows in the merged dataframe (which will be the number of identical rows)\n",
    "num_identical_rows = len(dp_df_merged)\n",
    "\n",
    "print(f'The two dataframes have {num_identical_rows} identical rows.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec7a7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def match_entities(df_ann, df_dp):\n",
    "    matches = []\n",
    "    for _, row_ann in df_ann.iterrows():\n",
    "        for _, row_dp in df_dp.iterrows():\n",
    "            if row_ann['Label'] == row_dp['Entity'] and fuzz.ratio(row_ann['Text'], row_dp['Text']) >= 80:\n",
    "                matches.append((row_ann.name, row_dp.name))\n",
    "                print(len(matches))\n",
    "    return matches\n",
    "\n",
    "matches = match_entities(ner_labeling_clean, dp_df)\n",
    "\n",
    "tp = len(matches)\n",
    "fp = len(dp_df) - tp\n",
    "fn = len(ner_labeling_clean) - tp\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * precision * recall / (precision + recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0d8d856d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_df = ner_labeling_clean\n",
    "recognized_df = dp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "2bb5a009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: ORG\n",
      "Precision: 0.8385991058122205\n",
      "Recall: 0.5516666666666666\n",
      "F1 score: 0.6655233589591957\n",
      "Class: LOC\n",
      "Precision: 0.4\n",
      "Recall: 0.11938775510204082\n",
      "F1 score: 0.18388998035363457\n",
      "Class: PER\n",
      "Precision: 0.11858704793944491\n",
      "Recall: 0.023809523809523808\n",
      "F1 score: 0.0396568696385881\n",
      "Overall precision: 0.6701885228920654\n",
      "Overall recall: 0.2940641759840015\n",
      "Overall F1 score: 0.4087692696487238\n"
     ]
    }
   ],
   "source": [
    "# Define the classes to calculate metrics for\n",
    "classes = [\"ORG\", \"LOC\", \"PER\"]\n",
    "\n",
    "# Calculate true positives, false positives, and false negatives for each class\n",
    "tp_by_class = {c: 0 for c in classes}\n",
    "fp_by_class = {c: 0 for c in classes}\n",
    "fn_by_class = {c: 0 for c in classes}\n",
    "for i, row in ground_truth_df.iterrows():\n",
    "    match_found = False\n",
    "    for match in matches:\n",
    "        if match[0] == i:\n",
    "            match_found = True\n",
    "            j = match[1]\n",
    "            if row[\"Label\"] == recognized_df.iloc[j][\"Entity\"]:\n",
    "                tp_by_class[row[\"Label\"]] += 1\n",
    "            else:\n",
    "                fn_by_class[row[\"Label\"]] += 1\n",
    "    if not match_found:\n",
    "        fn_by_class[row[\"Label\"]] += 1\n",
    "\n",
    "for j, row in recognized_df.iterrows():\n",
    "    match_found = False\n",
    "    for match in matches:\n",
    "        if match[1] == j:\n",
    "            match_found = True\n",
    "    if not match_found:\n",
    "        fp_by_class[row[\"Entity\"]] += 1\n",
    "\n",
    "# Calculate precision, recall, and F1 score for each class\n",
    "for c in classes:\n",
    "    precision = tp_by_class[c] / (tp_by_class[c] + fp_by_class[c])\n",
    "    recall = tp_by_class[c] / (tp_by_class[c] + fn_by_class[c])\n",
    "    f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    print(\"Class:\", c)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 score:\", f1_score)\n",
    "\n",
    "# Calculate overall precision, recall, and F1 score\n",
    "tp = sum(tp_by_class.values())\n",
    "fp = sum(fp_by_class.values())\n",
    "fn = sum(fn_by_class.values())\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(\"Overall precision:\", precision)\n",
    "print(\"Overall recall:\", recall)\n",
    "print(\"Overall F1 score:\", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6c59c1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B-LOC', 'B-ORG', 'B-PER', 'S-LOC', 'S-PER', 'S-ORG', 'E-ORG',\n",
       "       'I-ORG', 'I-PER', 'E-PER'], dtype=object)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_df['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b78231a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "natasha_df.to_excel('natasha_df.xlsx')\n",
    "spacy_df.to_excel('spacy_df.xlsx')\n",
    "dp_df.to_excel('dp_df.xlsx')\n",
    "ner_labeling_clean.to_excel('ner_labeling_clean.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
